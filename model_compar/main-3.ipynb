{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è¼‰å…¥å¥—ä»¶ å’Œå®šç¾©å‡½å¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dcard æ¨™ç±¤è¦å¤šæ³¨æ„ï¼Œå› ç‚ºå¾ˆå¸¸è¢«æ›´æ›"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import urllib.request as req\n",
    "import numpy as np\n",
    "from scipy.linalg import norm\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import bs4\n",
    "\n",
    "def tfidf_similarity(s1, s2):\n",
    "    def add_space(s):\n",
    "        return ' '.join(list(s))\n",
    "    # å°‡å­—ä¸­é–“åŠ å…¥ç©ºæ ¼\n",
    "    s1, s2 = add_space(s1), add_space(s2)\n",
    "    # è½‰åŒ–ç‚ºTFçŸ©é™£\n",
    "    cv = TfidfVectorizer(tokenizer=lambda s: s.split())\n",
    "    corpus = [s1, s2]\n",
    "    vectors = cv.fit_transform(corpus).toarray()\n",
    "    \n",
    "    # è¨ˆç®—TFä¿‚æ•¸\n",
    "    fiend = np.dot(vectors[0], vectors[1]) / (norm(vectors[0]) * norm(vectors[1])) \n",
    "    np.seterr(invalid='ignore') # ç•¶è¨ˆç®—çµæœç‚ºç„¡æ„ç¾©(åˆ†æ¯ç‚º0)ï¼Œå¿½ç•¥æ­¤è­¦å‘Š\n",
    "    return fiend\n",
    "    \n",
    "def stopwordslist(filepath):\n",
    "    stopwords = [line.strip() for line in open(filepath, 'r', encoding='utf-8').readlines()]\n",
    "    return stopwords\n",
    "\n",
    "def movestopwords(sentence):\n",
    "    stopwords = stopwordslist('stopword.txt')  # é€™è£åŠ è¼‰åœç”¨è©çš„è·¯å¾‘\n",
    "    outstr = ''\n",
    "    for word in sentence:           \n",
    "        if word not in stopwords:  \n",
    "            if word != '\\t'and'\\n':\n",
    "                outstr += word\n",
    "    return outstr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcardCraw(url):\n",
    "    # å»ºç«‹ä¸€å€‹Request ç‰©ä»¶ï¼Œé™„åŠ Request Headers çš„è³‡è¨Š\n",
    "    request = req.Request(url, headers={\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36\"\n",
    "    })\n",
    "    with req.urlopen(request) as response:\n",
    "        data = response.read().decode(\"utf-8\")\n",
    "    # ã€Œè§£æã€åŸå§‹ç¢¼ï¼Œå–å¾—æ¯ç¯‡æ–‡ç« çš„å•é¡Œ\n",
    "    # utf-8(æ¯”è¼ƒçœç©ºé–“)æœ‰éƒ¨åˆ†çš„æ¼¢å­—ä¸èƒ½è½‰æ›æ‰€ä»¥è¦ç”¨GB18030ç·¨ç¢¼\n",
    "\n",
    "    # è®“beautifulSoupå”åŠ©æˆ‘å€‘è§£æHTMLæ ¼å¼æ–‡ä»¶\n",
    "    root = bs4.BeautifulSoup(data, \"html.parser\")\n",
    "    titles = root.find(\"div\", class_=\"sc-ba53eaa8-0 hKkUKs\")  # ç”¨åˆ—è¡¨é¡¯ç¤ºå…¨éƒ¨çˆ¬èŸ²ä¸‹ä¾†çš„æ¨™é¡Œ\n",
    "\n",
    "    for title in titles:\n",
    "        result = title.text.strip().replace('\\n', '').replace(' ', '')\n",
    "        #å°å‡ºå…§æ–‡\n",
    "        print(result)\n",
    "    \n",
    "    return result\n",
    "    # titlesä»£è¡¨divæ¨™ç±¤\n",
    "    # å°‹æ‰¾class = \"title\" çš„div æ¨™ç±¤ï¼Œå› ç‚ºclassæ˜¯ä¿ç•™å­—ï¼Œæ‰€ä»¥å¯«æˆclass_\n",
    "    # root ä»£è¡¨æ•´å€‹ç¶²é ã€titleæ˜¯ç¶²é æ¨™ç±¤ä¹Ÿæ˜¯ç¶²é æ¨™é¡Œ\n",
    "    # cls æ˜¯æ¸…ç©ºçµ‚ç«¯æ©Ÿ(terminal)\n",
    "    # mode = \"a\"æ˜¯ä»¥é™„åŠ çš„æ–¹å¼æ‰“é–‹ä¸¦å¯«å…¥æ–‡ä»¶ï¼Œå› ç‚ºmode = \"w\"æœƒå°‡æª”æ¡ˆæ¸…ç©ºåœ¨å¯«å…¥ï¼Œmode=\"a\"ä¸æœƒæ¸…ç©º\n",
    "    \n",
    "def pttCraw(url):\n",
    "    #å»ºç«‹ä¸€å€‹Request ç‰©ä»¶ï¼Œé™„åŠ Request Headers çš„è³‡è¨Š\n",
    "    request = req.Request(url, headers={\n",
    "        \"cookie\":\"over18=1\",\n",
    "        \"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36\"\n",
    "    })\n",
    "    with req.urlopen(request) as response:\n",
    "        data = response.read().decode(\"utf-8\")\n",
    "\n",
    "    # print(data)\n",
    "    #ã€Œè§£æã€åŸå§‹ç¢¼ï¼Œå–å¾—æ¯ç¯‡æ–‡ç« çš„å•é¡Œ\n",
    "    # utf-8(æ¯”è¼ƒçœç©ºé–“)æœ‰éƒ¨åˆ†çš„æ¼¢å­—ä¸èƒ½è½‰æ›æ‰€ä»¥è¦ç”¨GB18030ç·¨ç¢¼\n",
    "\n",
    "    root = bs4.BeautifulSoup(data, \"html.parser\") # è®“beautifulSoupå”åŠ©æˆ‘å€‘è§£æHTMLæ ¼å¼æ–‡ä»¶\n",
    "    titles = root.find(\"div\", class_ = \"bbs-screen bbs-content\").text # ç”¨çˆ¬èŸ²æŠ“å…§æ–‡\n",
    "    \n",
    "    #å»é™¤æ‰ target_content\n",
    "    target_content = 'â€» ç™¼ä¿¡ç«™: æ‰¹è¸¢è¸¢å¯¦æ¥­åŠ(ptt.cc),'\n",
    "    content = titles.split(target_content)\n",
    "    \n",
    "    #å»é™¤æ‰ ä½œè€… çœ‹æ¿ æ¨™é¡Œ æ™‚é–“\n",
    "    results = root.select('span.article-meta-value')\n",
    "\n",
    "    if len(results)>3:\n",
    "        #ä½œè€… çœ‹æ¿ æ¨™é¡Œ æ™‚é–“\n",
    "        firstLine = \"ä½œè€…\" + results[0].text + \"çœ‹æ¿\" + results[1].text + \"æ¨™é¡Œ\" + results[2].text + \"æ™‚é–“\" + results[3].text\n",
    "\n",
    "    content = content[0].split(firstLine)\n",
    "    \n",
    "    #å»é™¤æ‰æ–‡æœ« --\n",
    "    main_content = content[1].replace('--', '')\n",
    "\n",
    "    #å»é™¤æ‰æ›è¡Œ\n",
    "    main_content = main_content.replace('\\n', '')\n",
    "    \n",
    "    #å°å‡ºå…§æ–‡\n",
    "    print(main_content)\n",
    "    \n",
    "    return main_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dcard API(æ²’ç”¨åˆ°)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# url = \"https://www.dcard.tw/service/api/v2/posts/238632575\"\n",
    "\n",
    "def get(url):\n",
    "    req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36'})\n",
    "    response = urllib.request.urlopen(req).read().decode('utf-8')\n",
    "    return response\n",
    "        \n",
    "def get_single_post(pid):\n",
    "    url = \"https://www.dcard.tw/service/api/v2/posts/{}\".format(pid)\n",
    "    reqsjson = json.loads(get(url)) \n",
    "    for title in reqsjson['content']:\n",
    "        result = title.replace('\\n', '').replace(' ', '') # .text.strip()\n",
    "    print(result) \n",
    "    return reqsjson\n",
    "\n",
    "get_single_post(238632575)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è¨ˆç®—é…é©åº¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('done_2021-08to12.csv')\n",
    "\n",
    "def find_song(url):\n",
    "    if url[12:15] == \"dca\":\n",
    "        article = dcardCraw(url)\n",
    "    else:\n",
    "        article = pttCraw(url)\n",
    "\n",
    "    lyrics=train['lyrics']\n",
    "    i=0\n",
    "    num=0\n",
    "    highpri=0\n",
    "    for text in lyrics:\n",
    "        text=movestopwords(text)\n",
    "        text=text.replace(' ','')\n",
    "        text=text.replace(',','ï¼Œ')\n",
    "        if tfidf_similarity(text, article)>highpri:\n",
    "            highpri=tfidf_similarity(text, article)\n",
    "            num=i\n",
    "        i+=1\n",
    "    print('é…é©åº¦:',highpri,'ä½œè€…:',train.singer.iloc[num],'æ­Œå:',train.name.iloc[num], 'æƒ…ç·’:',train.moodCat.iloc[num])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æŠ“å–æª”æ¡ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æˆ‘å…ˆâ€¦..1.æŠŠå†°ç®±é›è›‹ğŸ¥šå£“åœ¨æ•é ­ä¸‹å¸Œæœ›å­µå‡ºå°é›ğŸ¤ï¼ˆçµæœæƒ³ç•¶ç„¶çˆ¾æ˜¯è¢«æˆ‘å£“åˆ°çˆ›ç™¼è‡­å¾Œè¢«æˆ‘åª½ç™¼ç¾å°±è¢«è¿½è‘—æ‰“2.åœ¨å¤–å©†å®¶é–€å£è³£å¤–å©†çš„æ‹–é‹ä¸€é›™100$ï¼ˆå·®é»å®³å¤–å©†æ²’æ‹–é‹ç©¿â€¦3.å°æ™‚å€™ç‰¹æ„›è²·æˆ³æˆ³æ¨‚æˆ³äº†å¥½å¹¾ç›’å¾ŒæŠŠä¸å–œæ­¡çš„å°ç©å…·æ”¾åœ¨ç©ºçš„ä¿éº—é¾æˆ³æˆ³æ¨‚ç›’å­è²¼ä¸Šç™½ç´™ç•«æ ¼å­æ‹¿å»å­¸æ ¡è³£ä¸€æ ¼5$â€¦ï¼ˆè¢«è€å¸«ç™¼ç¾å¾Œå¯«è¯çµ¡ç°¿é€šçŸ¥å®¶é•·çµæœæ˜¯æˆ‘è¢«æ²’æ”¶äº†ä¸€å€‹ç¦®æ‹œçš„é›¶ç”¨éŒ¢â€¦4.è·Ÿå¼Ÿå¼Ÿèªªæˆ‘æ˜¯é­”å¥³ä¸å¯ä»¥è·Ÿåˆ¥äººè¬›é€™å€‹ç§˜å¯†ä¸ç„¶æˆ‘æœƒè®Šæˆé’è›™ğŸ¸ï¼ˆçµ•å°æ˜¯å°é­”å¥³æŠ–è•Šå’ªçœ‹å¤ªå¤šâ€¦.å‰›å‰›æ´—æ¾¡æ™‚å€™çªç™¼å¥‡æƒ³å°±æƒ³èµ·ä¾†é€™å››å€‹æˆ‘è¨˜å¾—å°æ™‚å€™é¬¼éˆç²¾æ€ªé¬¼é»å­è¶…å¤šï¼ä¸çŸ¥é“å¤§å®¶å°æ™‚å€™æœ‰æ²’æœ‰é¡ä¼¼é€™æ¨£å¾ˆå¥½ç©çš„äº‹æƒ…ğŸ¤£ğŸ¤£\n",
      "\n",
      "é€ä¸Šæˆ‘å®¶è‚¥æ©˜ğŸŠå¯«çœŸä¸€å¼µ\n",
      "\n",
      "\n",
      "é…é©åº¦: 0 ä½œè€…: é‡‘çŸå² æ­Œå: ä½ è¨»å®šæœƒé‡è¦‹æˆ‘ æƒ…ç·’: æ„›\n"
     ]
    }
   ],
   "source": [
    "# article = find_song('https://www.dcard.tw/f/relationship/p/238632575') #youtubeæœ‰\n",
    "# article = find_song('https://www.dcard.tw/f/talk/p/239984330') #youtubeæœ‰\n",
    "article = find_song('https://www.dcard.tw/f/talk/p/239983442') #youtubeæœ‰\n",
    "# article = find_song(\"http://www.ptt.cc/bbs/Boy-Girl/M.1664277279.A.9AA.html\") #youtubeæœ‰\n",
    "# article = find_song(\"https://www.ptt.cc/bbs/Gossiping/M.1664530650.A.4E3.html\") #youtubeæœ‰\n",
    "# article = find_song(\"http://www.ptt.cc/bbs/Boy-Girl/M.1660356781.A.365.html\") #youtubeæœ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9bf8e5f6ae5a440c6649c43ab49956741af2ee52909e232ddcd4abcc58504c21"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
